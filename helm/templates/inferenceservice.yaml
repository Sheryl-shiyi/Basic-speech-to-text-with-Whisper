apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: {{ .Values.model.name }}
  namespace: {{ .Values.namespaces.serving }}
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    automountServiceAccountToken: false
    minReplicas: {{ .Values.replicas.min }}
    maxReplicas: {{ .Values.replicas.max }}
    model:
      runtime: {{ .Values.model.name }}
      modelFormat:
        name: vLLM
      storageUri: {{ .Values.model.storageUri | quote }}
      resources:
        requests:
          cpu: {{ .Values.resources.requests.cpu | quote }}
          memory: {{ .Values.resources.requests.memory | quote }}
          nvidia.com/gpu: {{ .Values.resources.requests.gpu | quote }}
        limits:
          cpu: {{ .Values.resources.limits.cpu | quote }}
          memory: {{ .Values.resources.limits.memory | quote }}
          nvidia.com/gpu: {{ .Values.resources.limits.gpu | quote }}
