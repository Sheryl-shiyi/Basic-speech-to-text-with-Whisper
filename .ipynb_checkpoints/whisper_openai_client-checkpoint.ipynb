{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dfe5415",
   "metadata": {},
   "source": [
    "# Whisper Transcription via OpenAI Client (OpenAI‑compatible endpoint)\n",
    "\n",
    "This notebook demonstrates using the **OpenAI Python SDK** against an **OpenAI-compatible Whisper** service (e.g., your in-cluster service in OpenShift) to transcribe local audio files.\n",
    "\n",
    "**Highlights**\n",
    "- Uses `OpenAI` client (`openai` package) instead of manual `requests`.\n",
    "- Works with an endpoint that implements `/v1/audio/transcriptions`.\n",
    "- Small ipywidgets UI to browse, play, and transcribe files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbdd653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies if needed\n",
    "# You may already have these in your environment. Uncomment if necessary.\n",
    "# %pip install --quiet --upgrade openai ipywidgets\n",
    "# %pip install --quiet --upgrade soundfile  # sometimes required for Audio playback backends\n",
    "\n",
    "# If running in JupyterLab, enable widgets once (restart kernel might be required):\n",
    "# %pip install --quiet jupyterlab-widgets ipywidgets\n",
    "# %jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8177623",
   "metadata": {},
   "source": [
    "## 1) Configuration\n",
    "Adjust the host/paths to match your environment. Your service should be reachable from where this notebook runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b9f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# === CONFIG ===\n",
    "WHISPER_HOST = \"http://whisper-large-v3-predictor.whisper-proj.svc.cluster.local:8080\"  # cluster-internal service\n",
    "WHISPER_MODEL = \"whisper-large-v3\"  # your model identifier\n",
    "\n",
    "# Local audio directory to browse:\n",
    "LOCAL_AUDIO_DIR = Path(\"/opt/app-root/src/audio_data/\")\n",
    "\n",
    "# Audio extensions to include:\n",
    "AUDIO_EXTS = (\".wav\", \".mp3\", \".flac\", \".ogg\", \".m4a\", \".aac\")\n",
    "\n",
    "# If your service expects a bearer token, set it here or via environment variable OPENAI_API_KEY\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a062cb3d",
   "metadata": {},
   "source": [
    "## 2) Imports & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b14195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Any, Dict\n",
    "from IPython.display import Audio, display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from openai import OpenAI\n",
    "\n",
    "def list_local_audio_files(directory: Path, exts=AUDIO_EXTS) -> List[Path]:\n",
    "    directory = Path(directory)\n",
    "    return sorted([p for p in directory.glob(\"*\") if p.suffix.lower() in exts and p.is_file()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30bcdc",
   "metadata": {},
   "source": [
    "## 3) OpenAI client setup\n",
    "We point the client at the **OpenAI-compatible** base URL. Note we include `/v1` in the base URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da740a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_BASE_URL = f\"{WHISPER_HOST}/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=OPENAI_BASE_URL,\n",
    "    api_key=OPENAI_API_KEY,  # if your gateway ignores auth, empty string is fine\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc226673",
   "metadata": {},
   "source": [
    "## 4) Transcription helper (OpenAI client)\n",
    "This function calls `client.audio.transcriptions.create(...)`. You can pass optional Whisper parameters via `**extra`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b490917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_openai_client(\n",
    "    local_audio_path: Path,\n",
    "    model_name: str = WHISPER_MODEL,\n",
    "    **extra: Dict[str, Any],\n",
    ") -> str | dict:\n",
    "    \"\"\"\n",
    "    Uses the OpenAI Python SDK against an OpenAI-compatible Whisper endpoint.\n",
    "    Returns the 'text' when available; otherwise returns the raw object for inspection.\n",
    "    You can pass extras (e.g., language=\"de\", response_format=\"verbose_json\", temperature=0, prompt=\"...\").\n",
    "    \"\"\"\n",
    "    with open(local_audio_path, \"rb\") as f:\n",
    "        resp = client.audio.transcriptions.create(\n",
    "            model=model_name,\n",
    "            file=f,\n",
    "            **extra\n",
    "        )\n",
    "\n",
    "    # Many servers return an object with a .text field (OpenAI style)\n",
    "    text = getattr(resp, \"text\", None)\n",
    "    if text is not None:\n",
    "        return text\n",
    "\n",
    "    # Some adapters return plain dicts\n",
    "    if isinstance(resp, dict):\n",
    "        if \"text\" in resp:\n",
    "            return resp[\"text\"]\n",
    "        if \"choices\" in resp and resp[\"choices\"]:\n",
    "            choice = resp[\"choices\"][0]\n",
    "            if isinstance(choice, dict) and \"text\" in choice:\n",
    "                return choice[\"text\"]\n",
    "\n",
    "    return resp  # fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef8acbc",
   "metadata": {},
   "source": [
    "## 5) UI widgets\n",
    "A small UI to:\n",
    "- choose a folder\n",
    "- refresh file list\n",
    "- play a selected file\n",
    "- transcribe via the OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bb1be9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e076ceac884365b3d9c64266ddbe10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='/opt/app-root/src/audio_data', description='Folder:', layout=Layout(width='60%')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c113c5c1d1f244258fda6929c03afaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='File:', layout=Layout(width='70%'), options=(), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc9cd24d857411196aeee4a3aa1e4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Play', icon='play', style=ButtonStyle()), Button(description='Transcribe (O…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4e4e1caae74331b4d119a9cbbbdce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48b19e39d1a4d34ad63a689ca62bd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e074d526d9d44e39bde50ecda4b0607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inputs\n",
    "dir_text = widgets.Text(value=str(LOCAL_AUDIO_DIR), description=\"Folder:\", layout=widgets.Layout(width=\"60%\"))\n",
    "refresh_btn = widgets.Button(description=\"Refresh\", icon=\"refresh\")\n",
    "file_dd = widgets.Dropdown(options=[], description=\"File:\", layout=widgets.Layout(width=\"70%\"))\n",
    "\n",
    "# Actions\n",
    "play_btn = widgets.Button(description=\"Play\", icon=\"play\")\n",
    "transcribe_btn_openai = widgets.Button(description=\"Transcribe (OpenAI client)\", icon=\"microphone\")\n",
    "\n",
    "# Outputs\n",
    "status_out = widgets.Output()\n",
    "audio_out = widgets.Output()\n",
    "text_out = widgets.Output()\n",
    "\n",
    "def refresh_files(_=None):\n",
    "    folder = Path(dir_text.value).expanduser()\n",
    "    files = list_local_audio_files(folder)\n",
    "    file_dd.options = files\n",
    "    with status_out:\n",
    "        clear_output(wait=True)\n",
    "        if files:\n",
    "            print(f\"Found {len(files)} audio file(s) in {folder}\")\n",
    "        else:\n",
    "            print(f\"No audio files found in {folder}\")\n",
    "\n",
    "def play_audio(_=None):\n",
    "    sel = file_dd.value\n",
    "    if not sel:\n",
    "        return\n",
    "    with audio_out:\n",
    "        clear_output(wait=True)\n",
    "        display(Audio(filename=str(sel), autoplay=False))\n",
    "\n",
    "def run_transcription_openai(_=None):\n",
    "    sel = file_dd.value\n",
    "    if not sel:\n",
    "        return\n",
    "    with status_out:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Transcribing (OpenAI client): {Path(sel).name}\")\n",
    "    try:\n",
    "        txt = transcribe_with_openai_client(\n",
    "            Path(sel),\n",
    "            # Optional extras:\n",
    "            # language=\"sv\",  # force language if desired\n",
    "            # response_format=\"verbose_json\",  # or \"text\", \"json\", \"srt\", \"vtt\"\n",
    "            # temperature=0,\n",
    "            # prompt=\"Domain-specific hints here\",\n",
    "        )\n",
    "        with text_out:\n",
    "            clear_output(wait=True)\n",
    "            print(\"=== Transcript (OpenAI client) ===\")\n",
    "            print(txt if isinstance(txt, str) else str(txt))\n",
    "        with status_out:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Done.\")\n",
    "    except Exception as e:\n",
    "        with text_out:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Transcription failed (OpenAI client):\", e)\n",
    "\n",
    "refresh_btn.on_click(refresh_files)\n",
    "play_btn.on_click(play_audio)\n",
    "transcribe_btn_openai.on_click(run_transcription_openai)\n",
    "\n",
    "# Render the UI\n",
    "display(widgets.HBox([dir_text, refresh_btn]))\n",
    "display(file_dd)\n",
    "display(widgets.HBox([play_btn, transcribe_btn_openai]))\n",
    "display(status_out, audio_out, text_out)\n",
    "\n",
    "# Initial populate\n",
    "refresh_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77e803f",
   "metadata": {},
   "source": [
    "## 6) Quick smoke test cell (optional)\n",
    "Runs transcription on the currently selected file, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26209ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing via OpenAI client: Speaker_0007_00000.wav\n",
      "=== Transcript ===\n",
      " Hey everyone, this is Reshma from Edureka and today we'll be learning what is Ansible. Thank you all the attendees for joining today's session. So let's get started with it. First let us look at the topics that we'll be learning today. Well it's quite a long list, it means we'll be learning a lot of things today. Let us take a look at them one by one. So first we'll see the problems that were before configuration management and how configuration configuration management help to solve it. We'll see what Ansible is and the different features of Ansible. After that, we'll see how NASA has implemented Ansible to solve all their problems. After that, we'll see how we can use Ansible for orchestration, provisioning, configuration management, application deployment, and security. And in the end, we'll write some Ansible playbooks to install LAMP stack on my node machine and host a website in my node machine.\n"
     ]
    }
   ],
   "source": [
    "test_file = file_dd.value or (list_local_audio_files(Path(dir_text.value))[:1] or [None])[0]\n",
    "if test_file:\n",
    "    print(f\"Transcribing via OpenAI client: {Path(test_file).name}\")\n",
    "    try:\n",
    "        txt = transcribe_with_openai_client(Path(test_file))\n",
    "        print(\"=== Transcript ===\")\n",
    "        print(txt if isinstance(txt, str) else str(txt))\n",
    "    except Exception as e:\n",
    "        print(\"Transcription failed:\", e)\n",
    "else:\n",
    "    print(\"No audio file selected/found for quick test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1556aa5",
   "metadata": {},
   "source": [
    "## 7) (Optional) Async variant\n",
    "If you prefer `asyncio`, you can use `AsyncOpenAI`. Uncomment the cell content to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72fe715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import AsyncOpenAI\n",
    "# import asyncio\n",
    "# async_client = AsyncOpenAI(base_url=OPENAI_BASE_URL, api_key=OPENAI_API_KEY)\n",
    "#\n",
    "# async def atranscribe_with_openai_client(local_audio_path: Path, model_name: str = WHISPER_MODEL, **extra):\n",
    "#     with open(local_audio_path, \"rb\") as f:\n",
    "#         resp = await async_client.audio.transcriptions.create(\n",
    "#             model=model_name,\n",
    "#             file=f,\n",
    "#             **extra\n",
    "#         )\n",
    "#     return getattr(resp, \"text\", resp)\n",
    "#\n",
    "# # Example usage:\n",
    "# # await atranscribe_with_openai_client(Path(file_dd.value), language=\"sv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8f97b",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Ensure this environment can resolve and reach `whisper-...svc.cluster.local:8080`.\n",
    "- If your service requires auth, set `OPENAI_API_KEY` (env var or in the config cell).\n",
    "- Many Whisper adapters support extras like `language`, `temperature`, `prompt`, and `response_format` (`text`, `json`, `verbose_json`, `srt`, `vtt`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
