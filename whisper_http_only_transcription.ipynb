{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "461d96a3",
   "metadata": {},
   "source": [
    "# Whisper Transcription (HTTP-only) in OpenShift\n",
    "\n",
    "This notebook wraps a **minimal HTTP flow** for sending local audio files to your **OpenAI-compatible Whisper** service in OpenShift.\n",
    "\n",
    "**What you get**\n",
    "- Clear configuration section\n",
    "- Small helper to list audio files\n",
    "- A `requests`-based function that posts to `/v1/audio/transcriptions`\n",
    "- Simple ipywidgets UI to pick, play, and transcribe files\n",
    "\n",
    "> **Scope**: This version intentionally uses only the raw HTTP approach (no OpenAI SDK)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b4e97",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Run this where the service DNS (e.g., `*.svc.cluster.local`) is reachable.\n",
    "- Your Whisper service should implement `POST /v1/audio/transcriptions` (OpenAI-compatible).\n",
    "- Put a few test audio files in the configured folder (default: `/opt/app-root/src/audio_data/`).\n",
    "- If your endpoint requires auth, set an environment variable `OPENAI_API_KEY` before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff58a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies if needed\n",
    "# %pip install --quiet --upgrade requests ipywidgets\n",
    "# If running in JupyterLab, enable widgets once (restart kernel might be required):\n",
    "# %pip install --quiet jupyterlab-widgets ipywidgets\n",
    "# %jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f6edad",
   "metadata": {},
   "source": [
    "## 1) Configuration\n",
    "Update these values to match your environment and directory layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f6a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Whisper endpoint (adjust port/scheme to match your service)\n",
    "WHISPER_HOST = \"http://whisper-large-v3-predictor.whisper-proj.svc.cluster.local:8080\"\n",
    "WHISPER_API = f\"{WHISPER_HOST}/v1/audio/transcriptions\"\n",
    "\n",
    "# Model + auth (if any)\n",
    "WHISPER_MODEL = \"whisper-large-v3\"\n",
    "# Optional auth: set via env var OPENAI_API_KEY (or assign a token string here)\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Local audio directory (where your samples are)\n",
    "LOCAL_AUDIO_DIR = Path(\"/opt/app-root/src/audio_data/\")  # change to your path\n",
    "\n",
    "# Audio extensions to show in UI\n",
    "AUDIO_EXTS = (\".wav\", \".mp3\", \".flac\", \".ogg\", \".m4a\", \".aac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4256f329",
   "metadata": {},
   "source": [
    "## 2) Imports & Utilities\n",
    "We keep helpers small and focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d7fc92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS & UTILITIES ===\n",
    "import requests\n",
    "from typing import List\n",
    "from IPython.display import Audio\n",
    "\n",
    "def list_local_audio_files(directory: Path, exts=AUDIO_EXTS) -> List[Path]:\n",
    "    directory = Path(directory)\n",
    "    return sorted([p for p in directory.glob(\"*\") if p.suffix.lower() in exts and p.is_file()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9458c2c",
   "metadata": {},
   "source": [
    "## 3) Transcription function (HTTP via `requests`)\n",
    "Sends a **multipart/form-data** POST to the Whisper endpoint. Returns the transcript text when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e849664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_whisper(\n",
    "    local_audio_path: Path,\n",
    "    model_name: str = WHISPER_MODEL,\n",
    "    api_url: str = WHISPER_API,\n",
    "    api_key: str | None = OPENAI_API_KEY,\n",
    "    timeout: int = 180,\n",
    "):\n",
    "    \"\"\"Send a local file to an OpenAI-compatible Whisper endpoint and return transcript text (or raw JSON).\"\"\"\n",
    "    headers = {}\n",
    "    if api_key:\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "    \n",
    "    # Use a context manager to avoid leaving the file handle open\n",
    "    with open(local_audio_path, \"rb\") as f:\n",
    "        files = {\"file\": (local_audio_path.name, f, \"application/octet-stream\")}\n",
    "        data = {\"model\": model_name}\n",
    "        resp = requests.post(api_url, headers=headers, files=files, data=data, timeout=timeout)\n",
    "\n",
    "    resp.raise_for_status()\n",
    "    js = resp.json()\n",
    "    if \"text\" in js:\n",
    "        return js[\"text\"]\n",
    "    if isinstance(js, dict) and \"choices\" in js and js[\"choices\"] and \"text\" in js[\"choices\"][0]:\n",
    "        return js[\"choices\"][0][\"text\"]\n",
    "    return js  # fallback if the server returns a different shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e829cba",
   "metadata": {},
   "source": [
    "## 4) UI — Pick, play, and transcribe\n",
    "Use **Refresh** to scan the folder, **Play** to preview, and **Transcribe** to send the file to Whisper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92082fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bea0e98b074b8db744d6d2d32126a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='/opt/app-root/src/audio_data', description='Folder:', layout=Layout(width='60%')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c1ac7a4eb341988289addc4ca101e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='File:', layout=Layout(width='70%'), options=(), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1820bb051f24bf0a95b6d8a42d63051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Play', icon='play', style=ButtonStyle()), Button(description='Transcribe', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ebba6fc3914d84b4c6f8d438233876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d8a514b894400799aa2ecc32e632d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003ed86f2aa24ad58588367d70aaa745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === UI WIDGETS ===\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Inputs\n",
    "dir_text = widgets.Text(value=str(LOCAL_AUDIO_DIR), description=\"Folder:\", layout=widgets.Layout(width=\"60%\"))\n",
    "refresh_btn = widgets.Button(description=\"Refresh\", icon=\"refresh\")\n",
    "file_dd = widgets.Dropdown(options=[], description=\"File:\", layout=widgets.Layout(width=\"70%\"))\n",
    "\n",
    "# Actions\n",
    "play_btn = widgets.Button(description=\"Play\", icon=\"play\")\n",
    "transcribe_btn = widgets.Button(description=\"Transcribe\", icon=\"microphone\")\n",
    "\n",
    "# Outputs\n",
    "status_out = widgets.Output()\n",
    "audio_out = widgets.Output()\n",
    "text_out = widgets.Output()\n",
    "\n",
    "def refresh_files(_=None):\n",
    "    folder = Path(dir_text.value).expanduser()\n",
    "    files = list_local_audio_files(folder)\n",
    "    file_dd.options = files\n",
    "    with status_out:\n",
    "        clear_output(wait=True)\n",
    "        if files:\n",
    "            print(f\"Found {len(files)} audio file(s) in {folder}\")\n",
    "        else:\n",
    "            print(f\"No audio files found in {folder}\")\n",
    "\n",
    "def play_audio(_=None):\n",
    "    sel = file_dd.value\n",
    "    if not sel:\n",
    "        return\n",
    "    with audio_out:\n",
    "        clear_output(wait=True)\n",
    "        display(Audio(filename=str(sel), autoplay=False))\n",
    "\n",
    "def run_transcription(_=None):\n",
    "    sel = file_dd.value\n",
    "    if not sel:\n",
    "        return\n",
    "    with status_out:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Transcribing: {Path(sel).name}\")\n",
    "    try:\n",
    "        txt = transcribe_with_whisper(Path(sel))\n",
    "        with text_out:\n",
    "            clear_output(wait=True)\n",
    "            print(\"=== Transcript ===\")\n",
    "            print(txt if isinstance(txt, str) else str(txt))\n",
    "        with status_out:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Done.\")\n",
    "    except Exception as e:\n",
    "        with text_out:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Transcription failed:\", e)\n",
    "\n",
    "refresh_btn.on_click(refresh_files)\n",
    "play_btn.on_click(play_audio)\n",
    "transcribe_btn.on_click(run_transcription)\n",
    "\n",
    "# Render the UI\n",
    "display(widgets.HBox([dir_text, refresh_btn]))\n",
    "display(file_dd)\n",
    "display(widgets.HBox([play_btn, transcribe_btn]))\n",
    "display(status_out, audio_out, text_out)\n",
    "\n",
    "# initial file load\n",
    "refresh_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d75d59",
   "metadata": {},
   "source": [
    "## 5) (Optional) Quick smoke test\n",
    "Runs transcription on the currently selected file (or the first file in the folder if nothing is selected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24f4be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing (smoke test): Speaker_0007_00000.wav\n",
      "=== Transcript ===\n",
      " Hey everyone, this is Reshma from Edureka and today we'll be learning what is Ansible. Thank you all the attendees for joining today's session. So let's get started with it. First let us look at the topics that we'll be learning today. Well it's quite a long list, it means we'll be learning a lot of things today. Let us take a look at them one by one. So first we'll see the problems that were before configuration management and how configuration configuration management help to solve it. We'll see what Ansible is and the different features of Ansible. After that, we'll see how NASA has implemented Ansible to solve all their problems. After that, we'll see how we can use Ansible for orchestration, provisioning, configuration management, application deployment, and security. And in the end, we'll write some Ansible playbooks to install LAMP stack on my node machine and host a website in my node machine.\n"
     ]
    }
   ],
   "source": [
    "test_file = file_dd.value or (list_local_audio_files(LOCAL_AUDIO_DIR)[:1] or [None])[0]\n",
    "if test_file:\n",
    "    print(f\"Transcribing (smoke test): {Path(test_file).name}\")\n",
    "    try:\n",
    "        txt = transcribe_with_whisper(Path(test_file))\n",
    "        print(\"=== Transcript ===\")\n",
    "        print(txt if isinstance(txt, str) else str(txt))\n",
    "    except Exception as e:\n",
    "        print(\"Transcription failed:\", e)\n",
    "else:\n",
    "    print(\"No audio file found for smoke test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4dffd6",
   "metadata": {},
   "source": [
    "## 6) Troubleshooting\n",
    "- **Port & reachability**: Confirm `:8080` and that this notebook can reach `whisper-...svc.cluster.local`.\n",
    "- **Auth**: If your gateway requires a token, export `OPENAI_API_KEY` and re-run the config cell.\n",
    "- **Server errors**: Consider printing `resp.text` on non-2xx responses inside `transcribe_with_whisper` for more detail.\n",
    "- **Large files**: Increase `timeout` in `transcribe_with_whisper` as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
